# -*- coding: utf-8 -*-
"""Assignment 3 ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14rFObN5ZUB1zJu9r40tknXCLFaV1G0Il
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

data_path = '/content/drive/My Drive/dataset/assignment3_ml_data.csv'
data = pd.read_csv(data_path)

data

X = data.iloc[:, 1:].values / 255.0

# labels
y = data['label'].values

X.shape

len(y)

X = X / 255.0

input_dim = X.shape[1]
hidden_layer1 = 128
hidden_layer2 = 64
hidden_layer3 = 32
output_dim = len(np.unique(y))

input_dim, output_dim

np.random.seed(11)
weights = {
    'w1': np.random.randn(input_dim, hidden_layer1),
    'w2': np.random.randn(hidden_layer1, hidden_layer2),
    'w3': np.random.randn(hidden_layer2, hidden_layer3),
    'w4': np.random.randn(hidden_layer3, output_dim)
}

biases = {
    'b1': np.full((1, hidden_layer1), 11, dtype=np.float64),
    'b2': np.full((1, hidden_layer2), 11, dtype=np.float64),
    'b3': np.full((1, hidden_layer3), 11, dtype=np.float64),
    'b4': np.full((1, output_dim), 11, dtype=np.float64)
}

print(weights['w1'].shape) # dimensions (input_dim, hidden_layer1_size)
print(weights['w2'].shape) # dimensions (hidden_layer1_size, hidden_layer2_size)
print(weights['w3'].shape) # dimensions (hidden_layer2_size, hidden_layer3_size)
print(weights['w4'].shape) # # dimensions (hidden_layer3_size, output_dim)

print(
 (biases['b1'].shape),
 (biases['b2'].shape),
 (biases['b3'].shape),
 (biases['b4'].shape))

#activation energy
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# forward propagation
def forward_propagation(X, weights, biases):
    z1 = np.dot(X, weights['w1']) + biases['b1']
    a1 = sigmoid(z1)
    z2 = np.dot(a1, weights['w2']) + biases['b2']
    a2 = sigmoid(z2)
    z3 = np.dot(a2, weights['w3']) + biases['b3']
    a3 = sigmoid(z3)
    z4 = np.dot(a3, weights['w4']) + biases['b4']
    a4 = softmax(z4)

    return a1, a2, a3, a4

# backward propagation (backpropagation)
def backward_propagation(X, y, a1, a2, a3, a4, weights, biases, learning_rate):
    m = len(X)
    dz4 = a4 - np.eye(output_dim)[y]
    dw4 = np.dot(a3.T, dz4) / m
    db4 = np.sum(dz4, axis=0) / m

    dz3 = np.dot(dz4, weights['w4'].T) * a3 * (1 - a3)
    dw3 = np.dot(a2.T, dz3) / m
    db3 = np.sum(dz3, axis=0) / m

    dz2 = np.dot(dz3, weights['w3'].T) * a2 * (1 - a2)
    dw2 = np.dot(a1.T, dz2) / m
    db2 = np.sum(dz2, axis=0) / m

    dz1 = np.dot(dz2, weights['w2'].T) * a1 * (1 - a1)
    dw1 = np.dot(X.T, dz1) / m
    db1 = np.sum(dz1, axis=0) / m

    weights['w4'] -= learning_rate * dw4
    biases['b4'] -= learning_rate * db4
    weights['w3'] -= learning_rate * dw3
    biases['b3'] -= learning_rate * db3
    weights['w2'] -= learning_rate * dw2
    biases['b2'] -= learning_rate * db2
    weights['w1'] -= learning_rate * dw1
    biases['b1'] -= learning_rate * db1

# loss function (cross-entropy)
def cross_entropy_loss(y_true, y_pred):
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

# For 70:30 ratio epochs and running

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

epochs = 25
batch_size = 23
learning_rate = 0.1

accuracy_list = []
loss_list = []

for epoch in range(epochs):
    for i in range(0, len(X_train), batch_size):
        X_batch = X_train[i:i+batch_size]
        y_batch = y_train[i:i+batch_size]

        a1, a2, a3, a4 = forward_propagation(X_batch, weights, biases)
        backward_propagation(X_batch, y_batch, a1, a2, a3, a4, weights, biases, learning_rate)

    _, _, _, y_pred = forward_propagation(X_test, weights, biases)
    loss = cross_entropy_loss(np.eye(output_dim)[y_test], y_pred)

    accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test)
    accuracy_list.append(accuracy)
    loss_list.append(loss)

    print(f"Epoch {epoch+1}, Loss: {loss}")

# For 70:30 ratio
print("Dividing ratio of dataset", y_test.shape[0]/X_train.shape[0] )
print(f"Epoch {epoch+1}, Loss: {loss}")
average_accuracy = np.mean(accuracy_list)
print(f"Average Accuracy: {average_accuracy}")

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(range(1, epochs+1), accuracy_list, marker='o')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(range(1, epochs+1), loss_list, marker='o')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

_, _, _, y_pred = forward_propagation(X_test, weights, biases)
y_pred_classes = np.argmax(y_pred, axis=1)

conf_matrix = confusion_matrix(y_test, y_pred_classes)

print("Confusion Matrix for 70:30 ratio:\n\n", conf_matrix)

# For 80:20 ratio epochs and running

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

epochs = 25
batch_size = 23
learning_rate = 0.1

accuracy_list = []
loss_list = []

for epoch in range(epochs):
    for i in range(0, len(X_train), batch_size):
        X_batch = X_train[i:i+batch_size]
        y_batch = y_train[i:i+batch_size]

        a1, a2, a3, a4 = forward_propagation(X_batch, weights, biases)
        backward_propagation(X_batch, y_batch, a1, a2, a3, a4, weights, biases, learning_rate)

    _, _, _, y_pred = forward_propagation(X_test, weights, biases)
    loss = cross_entropy_loss(np.eye(output_dim)[y_test], y_pred)

    accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test)
    accuracy_list.append(accuracy)
    loss_list.append(loss)

    print(f"Epoch {epoch+1}, Loss: {loss}")

# For 80:20 ratio
print("Dividing ratio of dataset", y_test.shape[0]/X_train.shape[0] )
print(f"Epoch {epoch+1}, Loss: {loss}")
average_accuracy = np.mean(accuracy_list)
print(f"Average Accuracy: {average_accuracy}")

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(range(1, epochs+1), accuracy_list, marker='o')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(range(1, epochs+1), loss_list, marker='o')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

_, _, _, y_pred = forward_propagation(X_test, weights, biases)
y_pred_classes = np.argmax(y_pred, axis=1)

conf_matrix = confusion_matrix(y_test, y_pred_classes)

print("Confusion Matrix for 80:20 ratio:\n\n", conf_matrix)

# For 90:10 ratio epochs and running

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

accuracy_list = []
loss_list = []


epochs = 25
batch_size = 23
learning_rate = 0.1

for epoch in range(epochs):
    for i in range(0, len(X_train), batch_size):
        X_batch = X_train[i:i+batch_size]
        y_batch = y_train[i:i+batch_size]

        a1, a2, a3, a4 = forward_propagation(X_batch, weights, biases)
        backward_propagation(X_batch, y_batch, a1, a2, a3, a4, weights, biases, learning_rate)

    _, _, _, y_pred = forward_propagation(X_test, weights, biases)
    loss = cross_entropy_loss(np.eye(output_dim)[y_test], y_pred)


    accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test)
    accuracy_list.append(accuracy)
    loss_list.append(loss)


    print(f"Epoch {epoch+1}, Loss: {loss}")

# For 90:10 ratio
print("Dividing ratio of dataset", y_test.shape[0]/X_train.shape[0] )
print(f"Epoch {epoch+1}, Loss: {loss}")
average_accuracy = np.mean(accuracy_list)
print(f"Average Accuracy: {average_accuracy}")

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(range(1, epochs+1), accuracy_list, marker='o')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(range(1, epochs+1), loss_list, marker='o')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

_, _, _, y_pred = forward_propagation(X_test, weights, biases)
y_pred_classes = np.argmax(y_pred, axis=1)

conf_matrix = confusion_matrix(y_test, y_pred_classes)

print("Confusion Matrix for 90:10 ratio:\n\n", conf_matrix)

"""The trainable parameters of a neural network are typically the weights and biases that are modified throughout the training process. Non-trainable parameters are hyperparameters that are specified prior to training and stay fixed, such as learning rates or network design.



*   Input layer to Hidden layer 1: 785 (input features) x 128 (hidden units) + 128 (biases) = 100480 trainable parameters
*   Hidden layer 1 to Hidden layer 2: 128 x 64 + 64 = 8256 trainable parameters
*   Hidden layer 2 to Hidden layer 3: 64 x 32 + 32 = 2080 trainable parameters
*   Hidden layer 3 to Output layer: 32 x 10 + 10 = 330 trainable parameters


Total trainable parameters = 100480 + 8256 + 2080 + 330 = 111146






"""

total_trainable_params = (input_dim * hidden_layer1 + hidden_layer1 +
                          hidden_layer1 * hidden_layer2 + hidden_layer2 +
                          hidden_layer2 * hidden_layer3 + hidden_layer3 +
                          hidden_layer3 * output_dim + output_dim)

non_trainable_params = 4 * 11 + (hidden_layer1 + hidden_layer2 + hidden_layer3 + output_dim)

print(f"Total Trainable Parameters: {total_trainable_params}")
print(f"Total Non-trainable Parameters: {non_trainable_params}")